# Modeling Questions

Data modeling is where a data scientist provides value for a company. Turning data into predictive and actionable information is difficult, talking about it to a potential employer even more so. Practice describing your past experiences building models – what were the techniques used, challenges overcome, and successes achieved in the process? The group of questions below are designed to uncover that information, as well as your formal education of different modeling techniques. If you can’t describe the theory and assumptions associated with a model you’ve used, it won’t leave a good impression. 



Tell me about how you designed the model you created for a past employer or client.
What are your favorite data visualization techniques?
How would you effectively represent data with 5 dimensions? 
How is kNN different from k-means clustering?
kNN, or k-nearest neighbors is a classification algorithm, where the k is an integer describing the the number of neighboring data points that influence the classification of a given observation. K-means is a clustering algorithm, where the k is an integer describing the number of clusters to be created from the given data. Both accomplish different tasks.
How would you create a logistic regression model?
Have you used a time series model? Do you understand cross-correlations with time lags?
Explain the 80/20 rule, and tell me about its importance in model validation.
Explain what precision and recall are. How do they relate to the ROC curve?
Answer. Recall describes what percentage of true positives are described as positive by the model. Precision describes what percent of positive predictions were correct. The ROC curve shows the relationship between model recall and specificity – specificity being a measure of the percent of true negatives being described as negative by the model. Recall, precision, and the ROC are measures used to identify how useful a given classification model is.
Explain the difference between L1 and L2 regularization methods.
What is root cause analysis?
What are hash table collisions?
What is an exact test?
In your opinion, which is more important when designing a machine learning model: Model performance? Or model accuracy?
One approach to this question
What is one way that you would handle an imbalanced dataset that’s being used for prediction? (i.e. vastly more negative classes than positive classes.)
How would you validate a model you created to generate a predictive model of a quantitative outcome variable using multiple regression?
I have two models of comparable accuracy and computational performance. Which one should I choose for production and why?
How do you deal with sparsity?
Is it better to spend 5 days developing a 90% accurate solution, or 10 days for 100% accuracy?
What are some situations where a general linear model fails?
Do you think 50 small decision trees are better than a large one? Why?
When modifying an algorithm, how do you know that your changes are an improvement over not doing anything?
Is it better to have too many false positives, or too many false negatives?